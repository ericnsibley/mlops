{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./BankChurners.csv')\n",
    "df = df.drop(columns=['CLIENTNUM'])\n",
    "# XGBoost deals with missing values if I set them to np.Nan\n",
    "# I'm solving the problem with choice of algorithm since otherwise I'd either have to remove these or make guesses\n",
    "# Also XGBoost performs well on tabular data so I might've used it anyways\n",
    "columns_with_missing_labels = ['Education_Level', 'Marital_Status', 'Income_Category']\n",
    "df[columns_with_missing_labels] = df[columns_with_missing_labels].replace('Unknown', np.NaN)\n",
    "# df.isna().sum()\n",
    "\n",
    "df = df.rename(columns={\n",
    "    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1': 'NB1',\n",
    "    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2': 'NB2'\n",
    "})\n",
    "\n",
    "numerical_columns = ['Customer_Age', 'Dependent_count', 'Months_on_book', 'Total_Relationship_Count', \"Months_Inactive_12_mon\",\"Contacts_Count_12_mon\",\"Credit_Limit\",\"Total_Revolving_Bal\",\"Avg_Open_To_Buy\",\"Total_Amt_Chng_Q4_Q1\",\"Total_Trans_Amt\",\"Total_Trans_Ct\",\"Total_Ct_Chng_Q4_Q1\",\"Avg_Utilization_Ratio\",\"NB1\",\"NB2\"]\n",
    "scaler = StandardScaler()\n",
    "numerical_df = df[numerical_columns]\n",
    "scaler.fit(numerical_df)\n",
    "# Need to save the mean and std per col for receiving new input\n",
    "scaled_df = pd.DataFrame(scaler.transform(numerical_df), columns=numerical_df.columns)\n",
    "df[numerical_columns] = scaled_df\n",
    "\n",
    "categorical_columns = ['Education_Level', 'Marital_Status', 'Income_Category', 'Gender', 'Card_Category']\n",
    "encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "  df[col] = df[[col]].apply(encoder.fit_transform)\n",
    "df['Attrition_Flag'] = df['Attrition_Flag'].map({'Existing Customer': 0, 'Attrited Customer': 1})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Attrition_Flag']\n",
    "x = df.drop(columns=['Attrition_Flag'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tree = xgb.XGBClassifier()\n",
    "xgb_params = {}\n",
    "xgb_params['eval_metric'] = 'auc'\n",
    "xgb_params['early_stopping_rounds'] = 50\n",
    "xgb_tree.set_params(**xgb_params)\n",
    "xgb_tree.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_test, y_test)], verbose=True)\n",
    "print(f\"Best iteration: {xgb_tree.best_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_tree.predict_proba(x_test)\n",
    "predictions = np.argmax(y_pred, axis=1)\n",
    "# print(pd.Series(predictions).value_counts(normalize=True) * 100)\n",
    "# print(y_test.value_counts(normalize=True) * 100)\n",
    "auroc = roc_auc_score(y_test, predictions, multi_class=\"ovr\")\n",
    "print(f\"Evaluation completed with model accuracy: {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'totemic-guild-419402'\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user(project_id = PROJECT_ID)\n",
    "    print('Colab authorized to GCP')\n",
    "    !gcloud config get-value account\n",
    "    bq = bigquery.Client(project = PROJECT_ID)\n",
    "    bqstorage_client = bigquery_storage.BigQueryReadClient()\n",
    "except Exception:\n",
    "    print('Not a Colab Environment')\n",
    "    pass\n",
    "\n",
    "# BigQuery Parameters\n",
    "REGION = 'us-central1'\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = 'attrition_prevention'\n",
    "BQ_TABLE = 'bank_churners'\n",
    "BQ_REGION = REGION[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_normalize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  # df = df.drop(columns=['CLIENTNUM'])\n",
    "  df = df.drop(columns=['entity_id'])\n",
    "  # XGBoost deals with missing values if I set them to np.Nan\n",
    "  # I'm solving the problem with choice of algorithm since otherwise I'd either have to remove these or make guesses\n",
    "  # Also XGBoost performs well on tabular data so I might've used it anyways\n",
    "  columns_with_missing_labels = ['Education_Level', 'Marital_Status', 'Income_Category']\n",
    "  df[columns_with_missing_labels] = df[columns_with_missing_labels].replace('Unknown', np.NaN)\n",
    "  # df.isna().sum()\n",
    "\n",
    "  df = df.rename(columns={\n",
    "      'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1': 'NB1',\n",
    "      'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2': 'NB2'\n",
    "  })\n",
    "\n",
    "  numerical_columns = ['Customer_Age', 'Dependent_count', 'Months_on_book', 'Total_Relationship_Count', \"Months_Inactive_12_mon\",\"Contacts_Count_12_mon\",\"Credit_Limit\",\"Total_Revolving_Bal\",\"Avg_Open_To_Buy\",\"Total_Amt_Chng_Q4_Q1\",\"Total_Trans_Amt\",\"Total_Trans_Ct\",\"Total_Ct_Chng_Q4_Q1\",\"Avg_Utilization_Ratio\",\"NB1\",\"NB2\"]\n",
    "  scaler = StandardScaler()\n",
    "  numerical_df = df[numerical_columns]\n",
    "  scaler.fit(numerical_df)\n",
    "  # Need to save the mean and std per col for receiving new input\n",
    "  scaled_df = pd.DataFrame(scaler.transform(numerical_df), columns=numerical_df.columns)\n",
    "  df[numerical_columns] = scaled_df\n",
    "\n",
    "  categorical_columns = ['Education_Level', 'Marital_Status', 'Income_Category', 'Gender', 'Card_Category']\n",
    "  encoder = LabelEncoder()\n",
    "  for col in categorical_columns:\n",
    "    df[col] = df[[col]].apply(encoder.fit_transform)\n",
    "  df['Attrition_Flag'] = df['Attrition_Flag'].map({'Existing Customer': 0, 'Attrited Customer': 1})\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from BigQuery\n",
    "\n",
    "# Pagination is key with Big Data!\n",
    "query = f\"SELECT * FROM {BQ_DATASET}.{BQ_TABLE}\"\n",
    "job = bq.query(query)\n",
    "iterator = job.result(page_size=5_100)\n",
    "\n",
    "xgb_tree = xgb.XGBClassifier(\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=20\n",
    ")\n",
    "\n",
    "count = 0\n",
    "for page in iterator.pages:\n",
    "  df = pd.DataFrame([dict(row.items()) for row in page])\n",
    "  if df.empty:\n",
    "    continue\n",
    "\n",
    "  df = clean_and_normalize(df)\n",
    "\n",
    "  y = df['Attrition_Flag']\n",
    "  x = df.drop(columns=['Attrition_Flag'])\n",
    "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True)\n",
    "\n",
    "  # If I did not save between runs it would make a fresh tree on each call to .fit()!\n",
    "  if not count:\n",
    "    xgb_tree.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_test, y_test)], verbose=True)\n",
    "  else:\n",
    "    xgb_tree.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_test, y_test)], verbose=True, xgb_model='model.ubj')\n",
    "  xgb_tree.save_model('model.ubj')\n",
    "  print(f\"Best iteration: {xgb_tree.best_iteration}\")\n",
    "\n",
    "  y_pred = xgb_tree.predict_proba(x_test)\n",
    "  predictions = np.argmax(y_pred, axis=1)\n",
    "  # print(pd.Series(predictions).value_counts(normalize=True) * 100)\n",
    "  # print(y_test.value_counts(normalize=True) * 100)\n",
    "  auroc = roc_auc_score(y_test, predictions, multi_class=\"ovr\")\n",
    "  print(f\"Evaluation completed with model accuracy: {auroc}\")\n",
    "  count += 1\n",
    "\n",
    "print(f\"Total pages processed: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syncing your data:\n",
    "\n",
    "# In my mind, the data should trigger a sync which should\n",
    "# trigger the logic in this notebook\n",
    "# I do not see a way to trigger from their scheduler built into the feature store view\n",
    "# I am doing a pull semantic with a manual trigger here, but we should look into a push semantic\n",
    "\n",
    "# Paste this into the cloud shell to trigger a manual sync of a Feature View\n",
    "# https://cloud.google.com/vertex-ai/docs/featurestore/latest/sync-data#curl\n",
    "# Still pre-GA so they haven't built an API for it\n",
    "# There's the beta, then the beta of the beta, and that's where this API is\n",
    "# https://cloud.google.com/vertex-ai/docs/reference/rpc/google.cloud.aiplatform.v1beta1#google.cloud.aiplatform.v1beta1.FeaturestoreService\n",
    "\n",
    "# curl -X POST \\\n",
    "#     -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "#     -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "#     -d \"\" \\\n",
    "#     \"https://us-central1-aiplatform.googleapis.com/v1beta1/projects/totemic-guild-419402/locations/us-central1/featureOnlineStores/attrition_bigtable_store/featureViews/attrition_view:sync\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from Feature Store\n",
    "from google.cloud import aiplatform\n",
    "import google.auth\n",
    "\n",
    "creds, _ = google.auth.default()\n",
    "feature_store_client = aiplatform.gapic.FeatureOnlineStoreAdminServiceClient(credentials=creds, client_options = dict(api_endpoint = f'{REGION}-aiplatform.googleapis.com'))\n",
    "\n",
    "FEATURE_VIEW_NAME = 'attrition_view'\n",
    "FEATURE_ONLINE_STORE_NAME = 'attrition_bigtable_store'\n",
    "proj_id = ''\n",
    "\n",
    "online_store = feature_store_client.get_feature_online_store(name = f'projects/{PROJECT_ID}/locations/{REGION}/featureOnlineStores/{FEATURE_ONLINE_STORE_NAME}')\n",
    "\n",
    "online_store.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = feature_store_client.get_feature_view(name = f'{online_store.name}/featureViews/{FEATURE_VIEW_NAME}')\n",
    "feature_view.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_serve_client = aiplatform.gapic.FeatureOnlineStoreServiceClient(client_options = dict(api_endpoint = f'{REGION}-aiplatform.googleapis.com'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that I had to hardcode the id of the row that I'm pulling; how does the prediction service know which ID to refresh & pull? \n",
    "# That means it must not; prediction endpoint is meant to be sent at\n",
    "# Use airflow to trigger syncs, get ids, then send request to prediction endpoint? \n",
    "\n",
    "# Also only one row at a time means this is prediction only; no training with this \n",
    "from google.cloud.aiplatform_v1 import FeatureOnlineStoreServiceClient\n",
    "from google.cloud.aiplatform_v1.types import feature_online_store_service as feature_online_store_service_pb2\n",
    "\n",
    "data_client = FeatureOnlineStoreServiceClient(\n",
    "  client_options={\"api_endpoint\": f'{REGION}-aiplatform.googleapis.com'}\n",
    ")\n",
    "dict(data_client.fetch_feature_values(\n",
    "  request=feature_online_store_service_pb2.FetchFeatureValuesRequest(\n",
    "    feature_view=feature_view.name,\n",
    "    data_key=feature_online_store_service_pb2.FeatureViewDataKey(key = '826077033'),\n",
    "    data_format=feature_online_store_service_pb2.FeatureViewDataFormat.PROTO_STRUCT,\n",
    "  )\n",
    ").proto_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.featurestore import Featurestore\n",
    "\n",
    "# Want to see a typo in a production google api? Uncomment the line below\n",
    "featurestore = Featurestore(featurestore_name=online_store.name)\n",
    "\n",
    "featurestore = Featurestore(featurestore_name='projects/614098673135/locations/us-central1/featurestores/attrition_bigtable_store')\n",
    "featurestore"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
